{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4db9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a52249bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  label                                          statement  \\\n",
      "0  13847      5  90 percent of Americans \"support universal bac...   \n",
      "1  13411      1  Last year was one of the deadliest years ever ...   \n",
      "2  10882      0  Bernie Sanders's plan is \"to raise your taxes ...   \n",
      "3  20697      4  Voter ID is supported by an overwhelming major...   \n",
      "4   6095      2  Says Barack Obama \"robbed Medicare (of) $716 b...   \n",
      "\n",
      "               date                                            subject  \\\n",
      "0   October 2, 2017  government regulation;polls and public opinion...   \n",
      "1      May 19, 2017  after the fact;congress;criminal justice;histo...   \n",
      "2  October 28, 2015                                              taxes   \n",
      "3  December 8, 2021                                      voter id laws   \n",
      "4   August 12, 2012         federal budget;history;medicare;retirement   \n",
      "\n",
      "          speaker                                speaker_description  \\\n",
      "0     chris abele  Chris Abele is Milwaukee County Executive, a p...   \n",
      "1     thom tillis  Thom Tillis is a Republican who serves as U.S....   \n",
      "2  chris christie  Chris Christie announced June 6, 2023 that he ...   \n",
      "3      lee zeldin  Lee Zeldin is a Republican representing New Yo...   \n",
      "4     mitt romney  Mitt Romney is a U.S. senator from Utah. He ra...   \n",
      "\n",
      "       state_info  true_counts  mostly_true_counts  half_true_counts  \\\n",
      "0       wisconsin            1                   4                 5   \n",
      "1  north carolina            0                   2                 7   \n",
      "2        national           21                  20                27   \n",
      "3        new york            1                   2                 0   \n",
      "4        national           31                  33                58   \n",
      "\n",
      "   mostly_false_counts  false_counts  pants_on_fire_counts  \\\n",
      "0                    3             5                     2   \n",
      "1                    3             2                     0   \n",
      "2                   11            17                     8   \n",
      "3                    0             0                     0   \n",
      "4                   35            32                    19   \n",
      "\n",
      "                                             context  \\\n",
      "0                                            a tweet   \n",
      "1  a press release supporting the Back The Blue A...   \n",
      "2                                      Boulder, Colo   \n",
      "3                                            a Tweet   \n",
      "4                       an interview on \"60 Minutes\"   \n",
      "\n",
      "                                       justification  \n",
      "0  \"Universal\" is the term for background checks ...  \n",
      "1  Sen. Thom Tillis, a North Carolina Republican,...  \n",
      "2  Christie said that Sanders’s plan is \"to raise...  \n",
      "3  Zeldin claimed voter identification requiremen...  \n",
      "4  Romney said, \"There's only one president that ...  \n",
      "label\n",
      "1    5284\n",
      "3    2967\n",
      "2    2882\n",
      "4    2743\n",
      "0    2425\n",
      "5    2068\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load LIAR-2 CSVs\n",
    "train_df = pd.read_csv('LIAR2/train.csv', encoding='utf-8')\n",
    "valid_df = pd.read_csv('LIAR2/valid.csv', encoding='utf-8')\n",
    "test_df = pd.read_csv('LIAR2/test.csv', encoding='utf-8')\n",
    "\n",
    "# Column names for LIAR-2\n",
    "columns = [\n",
    "    \"id\", \"label\", \"statement\", \"date\", \"subject\", \"speaker\", \"speaker_description\", \"state_info\",\n",
    "    \"true_counts\", \"mostly_true_counts\", \"half_true_counts\", \"mostly_false_counts\",\n",
    "    \"false_counts\", \"pants_on_fire_counts\", \"context\", \"justification\"\n",
    "]\n",
    "\n",
    "# Assign to DataFrames\n",
    "train_df.columns = columns\n",
    "valid_df.columns = columns\n",
    "test_df.columns = columns\n",
    "\n",
    "# ✅ Check\n",
    "print(train_df.head())\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a583d925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18369, 5000) (2297, 5000) (2296, 5000)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download required NLTK data once (run this once in your environment)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text).lower().strip()\n",
    "    # Remove non-alphabetic characters except spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "train_df['statement'] = train_df['statement'].apply(preprocess)\n",
    "valid_df['statement'] = valid_df['statement'].apply(preprocess)\n",
    "test_df['statement'] = test_df['statement'].apply(preprocess)\n",
    "\n",
    "# Create TF-IDF vectorizer (no need for stop_words param since already removed)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(train_df['statement'])\n",
    "X_valid_tfidf = vectorizer.transform(valid_df['statement'])\n",
    "X_test_tfidf = vectorizer.transform(test_df['statement'])\n",
    "\n",
    "print(X_train_tfidf.shape, X_valid_tfidf.shape, X_test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9075a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label(label):\n",
    "    # 3: half-true, 4: mostly-true, 5: true → 1 (true)\n",
    "    # 0: pants-fire, 1: false, 2: barely-true → 0 (false)\n",
    "    return 1 if label in [3, 4, 5] else 0\n",
    "\n",
    "train_df['binary_label'] = train_df['label'].apply(map_label)\n",
    "valid_df['binary_label'] = valid_df['label'].apply(map_label)\n",
    "test_df['binary_label'] = test_df['label'].apply(map_label)\n",
    "\n",
    "y_train = train_df['binary_label'].values\n",
    "y_valid = valid_df['binary_label'].values\n",
    "y_test = test_df['binary_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca5dc00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution: [10591  7778]\n",
      "Validation label distribution: [1325  972]\n",
      "Test label distribution: [1323  973]\n"
     ]
    }
   ],
   "source": [
    "# Check if both classes are present in each split\n",
    "print(\"Train label distribution:\", np.bincount(y_train))\n",
    "print(\"Validation label distribution:\", np.bincount(y_valid))\n",
    "print(\"Test label distribution:\", np.bincount(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2385765",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed5b1222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6586852416195037\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       false       0.73      0.64      0.68      1325\n",
      "        true       0.58      0.68      0.63       972\n",
      "\n",
      "    accuracy                           0.66      2297\n",
      "   macro avg       0.66      0.66      0.66      2297\n",
      "weighted avg       0.67      0.66      0.66      2297\n",
      "\n",
      "Test Accuracy: 0.6589721254355401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       false       0.73      0.64      0.69      1323\n",
      "        true       0.58      0.68      0.63       973\n",
      "\n",
      "    accuracy                           0.66      2296\n",
      "   macro avg       0.66      0.66      0.66      2296\n",
      "weighted avg       0.67      0.66      0.66      2296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Train the model\n",
    "clf = LinearSVC(class_weight='balanced', random_state=42)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Validate\n",
    "y_valid_pred = clf.predict(X_valid_tfidf)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_valid, y_valid_pred))\n",
    "print(classification_report(y_valid, y_valid_pred, target_names=['false', 'true']))\n",
    "\n",
    "# Test\n",
    "y_test_pred = clf.predict(X_test_tfidf)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred, target_names=['false', 'true']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30bc782",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f21aadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Validation Accuracy: 0.6769699608184588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       false       0.71      0.75      0.73      1325\n",
      "        true       0.63      0.58      0.60       972\n",
      "\n",
      "    accuracy                           0.68      2297\n",
      "   macro avg       0.67      0.66      0.67      2297\n",
      "weighted avg       0.67      0.68      0.67      2297\n",
      "\n",
      "LR Test Accuracy: 0.6803135888501742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       false       0.71      0.76      0.73      1323\n",
      "        true       0.64      0.57      0.60       973\n",
      "\n",
      "    accuracy                           0.68      2296\n",
      "   macro avg       0.67      0.67      0.67      2296\n",
      "weighted avg       0.68      0.68      0.68      2296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_lr = LogisticRegression(max_iter=1000)\n",
    "clf_lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"LR Validation Accuracy:\", accuracy_score(y_valid, clf_lr.predict(X_valid_tfidf)))\n",
    "print(classification_report(y_valid, clf_lr.predict(X_valid_tfidf), target_names=['false', 'true']))\n",
    "\n",
    "print(\"LR Test Accuracy:\", accuracy_score(y_test, clf_lr.predict(X_test_tfidf)))\n",
    "print(classification_report(y_test, clf_lr.predict(X_test_tfidf), target_names=['false', 'true']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d67ab14",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54a8b0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Validation Accuracy: 0.6752285589899869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       false       0.72      0.71      0.72      1325\n",
      "        true       0.61      0.62      0.62       972\n",
      "\n",
      "    accuracy                           0.68      2297\n",
      "   macro avg       0.67      0.67      0.67      2297\n",
      "weighted avg       0.68      0.68      0.68      2297\n",
      "\n",
      "RF Test Accuracy: 0.669425087108014\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       false       0.71      0.72      0.71      1323\n",
      "        true       0.61      0.60      0.61       973\n",
      "\n",
      "    accuracy                           0.67      2296\n",
      "   macro avg       0.66      0.66      0.66      2296\n",
      "weighted avg       0.67      0.67      0.67      2296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_rf.fit(X_train_tfidf, y_train)\n",
    "print(\"RF Validation Accuracy:\", accuracy_score(y_valid, clf_rf.predict(X_valid_tfidf)))\n",
    "print(classification_report(y_valid, clf_rf.predict(X_valid_tfidf), target_names=['false', 'true']))\n",
    "print(\"RF Test Accuracy:\", accuracy_score(y_test, clf_rf.predict(X_test_tfidf)))\n",
    "print(classification_report(y_test, clf_rf.predict(X_test_tfidf), target_names=['false', 'true']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baebe277",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efc3bc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5743, Accuracy: 0.6774\n",
      "Epoch [2/10], Loss: 0.5793, Accuracy: 0.6822\n",
      "Epoch [3/10], Loss: 0.6137, Accuracy: 0.6665\n",
      "Epoch [4/10], Loss: 0.6537, Accuracy: 0.6657\n",
      "Epoch [5/10], Loss: 0.7117, Accuracy: 0.6778\n",
      "Epoch [6/10], Loss: 0.7940, Accuracy: 0.6609\n",
      "Epoch [7/10], Loss: 0.8960, Accuracy: 0.6652\n",
      "Epoch [8/10], Loss: 1.0368, Accuracy: 0.6670\n",
      "Epoch [9/10], Loss: 1.1570, Accuracy: 0.6613\n",
      "Epoch [10/10], Loss: 1.2744, Accuracy: 0.6569\n",
      "Test Loss: 1.2476, Test Accuracy: 0.6555\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       false       0.71      0.69      0.70      1323\n",
      "        true       0.59      0.61      0.60       973\n",
      "\n",
      "    accuracy                           0.66      2296\n",
      "   macro avg       0.65      0.65      0.65      2296\n",
      "weighted avg       0.66      0.66      0.66      2296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# deeplearning using pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
    "X_valid_tensor = torch.tensor(X_valid_tfidf.toarray(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_valid_tensor = torch.tensor(y_valid, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "# Define a simple feedforward neural network\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "output_dim = 2  # For binary classification\n",
    "model = SimpleNN(input_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {valid_loss/len(valid_loader):.4f}, Accuracy: {correct/total:.4f}')\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "print(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {correct/total:.4f}')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, all_preds, target_names=['false', 'true']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ad219",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237e6a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5774, Accuracy: 0.6865\n",
      "Epoch [2/10], Loss: 0.5740, Accuracy: 0.6778\n",
      "Epoch [3/10], Loss: 0.5943, Accuracy: 0.6748\n",
      "Epoch [4/10], Loss: 0.6144, Accuracy: 0.6704\n",
      "Epoch [5/10], Loss: 0.6377, Accuracy: 0.6726\n",
      "Epoch [6/10], Loss: 0.6538, Accuracy: 0.6674\n",
      "Epoch [7/10], Loss: 0.6713, Accuracy: 0.6678\n",
      "Epoch [8/10], Loss: 0.6932, Accuracy: 0.6643\n",
      "Epoch [9/10], Loss: 0.7075, Accuracy: 0.6683\n",
      "Epoch [10/10], Loss: 0.7186, Accuracy: 0.6674\n",
      "Test Loss: 0.7018, Test Accuracy: 0.6585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       false       0.70      0.71      0.70      1323\n",
      "        true       0.60      0.59      0.60       973\n",
      "\n",
      "    accuracy                           0.66      2296\n",
      "   macro avg       0.65      0.65      0.65      2296\n",
      "weighted avg       0.66      0.66      0.66      2296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lstm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Use the last time step\n",
    "        return out\n",
    "    \n",
    "# Initialize the LSTM model, loss function, and optimizer\n",
    "hidden_dim = 64\n",
    "lstm_model = LSTMClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for LSTM\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    lstm_model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    lstm_model.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            outputs = lstm_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {valid_loss/len(valid_loader):.4f}, Accuracy: {correct/total:.4f}')\n",
    "\n",
    "# Testing the LSTM model\n",
    "lstm_model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = lstm_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "print(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {correct/total:.4f}')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, all_preds, target_names=['false', 'true']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6fe2dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.7.0+cu118\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed554edd",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71734c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe524813b79142969f6299216b23eb7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d335868c1e4ab7b5e3cace96babb07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fedb1e5ff23b406d9eb75e3cc6918637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8364fd7e2094056a7e39e28c50602eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca0ef553ba54ea787c1c63e98687d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add BERT model for binary classification\n",
    "# Install transformers if not already installed: pip install transformers\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import torch\n",
    "\n",
    "class WeightedBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def __init__(self, config, class_weights=None):\n",
    "        super().__init__(config)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Calculate weights, e.g.:\n",
    "import numpy as np\n",
    "class_counts = np.bincount(y_train)\n",
    "weights = torch.tensor(1.0 / class_counts, dtype=torch.float)\n",
    "weights = weights / weights.sum()  # Normalize\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_len)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds, average='weighted'),\n",
    "        'precision': precision_score(labels, preds, average='weighted'),\n",
    "        'recall': recall_score(labels, preds, average='weighted')\n",
    "    }\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = NewsDataset(train_df['statement'].tolist(), y_train, tokenizer)\n",
    "valid_dataset = NewsDataset(valid_df['statement'].tolist(), y_valid, tokenizer)\n",
    "test_dataset = NewsDataset(test_df['statement'].tolist(), y_test, tokenizer)\n",
    "\n",
    "model_bert = WeightedBertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', num_labels=2, class_weights=weights\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_bert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"BERT Validation Accuracy:\", eval_results['eval_accuracy'])\n",
    "\n",
    "# Test set evaluation\n",
    "test_results = trainer.predict(test_dataset)\n",
    "bert_test_acc = accuracy_score(y_test, test_results.predictions.argmax(axis=1))\n",
    "print(\"BERT Test Accuracy:\", bert_test_acc)\n",
    "print(classification_report(y_test, test_results.predictions.argmax(axis=1), target_names=['false','true']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9999eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('best_model_bert\\\\tokenizer_config.json',\n",
       " 'best_model_bert\\\\special_tokens_map.json',\n",
       " 'best_model_bert\\\\vocab.txt',\n",
       " 'best_model_bert\\\\added_tokens.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bert.save_pretrained(\"best_model_bert\")\n",
    "tokenizer.save_pretrained(\"best_model_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11211279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: logreg with accuracy 0.6803\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Collect test accuracies\n",
    "results = {\n",
    "    \"svm\": accuracy_score(y_test, y_test_pred),\n",
    "    \"logreg\": accuracy_score(y_test, clf_lr.predict(X_test_tfidf)),\n",
    "    \"simplenn\": correct / total,  # from SimpleNN test\n",
    "    \"lstm\": correct / total,      # from LSTM test\n",
    "    \"bert\": bert_test_acc,        # from BERT test\n",
    "}\n",
    "\n",
    "# Find the best model\n",
    "best_model_name = max(results, key=results.get)\n",
    "print(f\"Best model: {best_model_name} with accuracy {results[best_model_name]:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "if best_model_name == \"svm\":\n",
    "    joblib.dump(clf, \"best_model_svm.joblib\")\n",
    "elif best_model_name == \"logreg\":\n",
    "    joblib.dump(clf_lr, \"best_model_logreg.joblib\")\n",
    "elif best_model_name == \"simplenn\":\n",
    "    torch.save(model.state_dict(), \"best_model_simplenn.pt\")\n",
    "elif best_model_name == \"lstm\":\n",
    "    torch.save(lstm_model.state_dict(), \"best_model_lstm.pt\")\n",
    "elif best_model_name == \"bert\":\n",
    "    model_bert.save_pretrained(\"best_model_bert\")\n",
    "    tokenizer.save_pretrained(\"best_model_bert\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
